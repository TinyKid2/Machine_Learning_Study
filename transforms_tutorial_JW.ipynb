{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sRDL3RuxP1PW"
   },
   "outputs": [],
   "source": [
    "# Google Colab에서 노트북을 실행하실 때에는\n",
    "# https://tutorials.pytorch.kr/beginner/colab 를 참고하세요.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxDrhe-DP1PY"
   },
   "source": [
    "\n",
    "[파이토치(PyTorch) 기본 익히기](intro.html) ||\n",
    "[빠른 시작](quickstart_tutorial.html) ||\n",
    "[텐서(Tensor)](tensorqs_tutorial.html) ||\n",
    "[Dataset과 Dataloader](data_tutorial.html) ||\n",
    "**변형(Transform)** ||\n",
    "[신경망 모델 구성하기](buildmodel_tutorial.html) ||\n",
    "[Autograd](autogradqs_tutorial.html) ||\n",
    "[최적화(Optimization)](optimization_tutorial.html) ||\n",
    "[모델 저장하고 불러오기](saveloadrun_tutorial.html)\n",
    "\n",
    "# 변형(Transform)\n",
    "\n",
    "데이터가 항상 머신러닝 알고리즘 학습에 필요한 최종 처리가 된 형태로 제공되지는 않습니다.\n",
    "**변형(transform)** 을 해서 데이터를 조작하고 학습에 적합하게 만듭니다.\n",
    "\n",
    "모든 TorchVision 데이터셋들은 변형 로직을 갖는, 호출 가능한 객체(callable)를 받는 매개변수 두개\n",
    "( 특징(feature)을 변경하기 위한 ``transform`` 과 정답(label)을 변경하기 위한 ``target_transform`` )를 갖습니다\n",
    "[torchvision.transforms](https://pytorch.org/vision/stable/transforms.html) 모듈은\n",
    "주로 사용하는 몇가지 변형(transform)을 제공합니다.\n",
    "\n",
    "FashionMNIST 특징(feature)은 PIL Image 형식이며, 정답(label)은 정수(integer)입니다.\n",
    "학습을 하려면 정규화(normalize)된 텐서 형태의 특징(feature)과 원-핫(one-hot)으로 부호화(encode)된 텐서 형태의\n",
    "정답(label)이 필요합니다. 이러한 변형(transformation)을 하기 위해 ``ToTensor`` 와 ``Lambda`` 를 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7w7E80AlP1PZ",
    "outputId": "5f688a77-422b-4429-e66d-fdfc218f4d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 16975066.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 330584.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 6141921.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 6158664.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),#label 값은 0-9까지 숫자\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "# 텐서에 0으로 가득찬 텐서 제작 -> 텐서에 해당되는 값만 1로 고정 -> onehot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8HG1FmGP1PZ"
   },
   "source": [
    "## ToTensor()\n",
    "\n",
    "[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor)\n",
    "는 PIL Image나 NumPy ``ndarray`` 를  ``FloatTensor`` 로 변환하고, 이미지의 픽셀의 크기(intensity) 값을 [0., 1.] 범위로\n",
    "비례하여 조정(scale)합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgjgYm1PQXnj",
    "outputId": "b32c7a24-815b-4fc2-c351-83735bdb53ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4]).float() # 이곳 변형 # tensor 형태로 가공 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vk63YTGP1Pa"
   },
   "source": [
    "## Lambda 변형(Transform)\n",
    "\n",
    "Lambda 변형은 사용자 정의 람다(lambda) 함수를 적용합니다. 여기에서는 정수를 원-핫으로 부호화된 텐서로 바꾸는\n",
    "함수를 정의합니다.\n",
    "이 함수는 먼저 (데이터셋 정답의 개수인) 크기 10짜리 영 텐서(zero tensor)를 만들고,\n",
    "[scatter_](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html) 를 호출하여\n",
    "주어진 정답 ``y`` 에 해당하는 인덱스에 ``value=1`` 을 할당합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iC7qEZJQpzy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xyygnwo4P1Pa"
   },
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TVjQygAQ1ci",
    "outputId": "3549510d-abd2-42b3-8d1f-5a3f35a8d224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "2 tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "3 tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "4 tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "5 tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "6 tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "7 tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,8): # 이곳 변형\n",
    "  print(i,target_transform(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM5KmGEnRF4r"
   },
   "source": [
    "# 이것을 해주는 이유 :\n",
    "- 로스값을 계산해줄때 loss 함수를 변형해주어야 한다.\n",
    "- loss 계산 -> softmax값으로 나올때, onehot encoding형태일때 계산이 편리하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVkYuu7MP1Pa"
   },
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSKmFWR9P1Pa"
   },
   "source": [
    "### 더 읽어보기\n",
    "- [torchvision.transforms API](https://pytorch.org/vision/stable/transforms.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
